x-llama-build: &llama-build
  build:
    context: .
    dockerfile: docker/Dockerfile.llamacpp
  image: local/llama.cpp:server-cuda

services:
  dataprep:
    command: ["dataprep_server", "--config", "configs/config-docker-dgx.yaml"]
    volumes:
      - ./test_files:/app/test_files

  agentic-research:
    command: ["agentic-research", "--config", "configs/config-docker-dgx.yaml"]
    depends_on:
      - dataprep
      - chromadb
      - llm-instruct
      - llm-reasoning
    volumes:
      - ./test_files:/app/test_files

  chromadb:
    image: chromadb/chroma:1.4.1
    ports:
      - "8000:8000"
    volumes:
      - ./data/chromadb:/data
    environment:
      ANONYMIZED_TELEMETRY: "False"

  embeddings-gpu:
    <<: *llama-build
    volumes:
      - ${MODELS_DIR:-${HOME}/.cache/huggingface/hub}:/mnt/models
    environment:
      EXTRA_PARAMS: "${EMBEDDINGS_EXTRA_PARAMS-}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/app/llama-server-entrypoint.sh"]
    command:
      - "-m"
      - "${EMBEDDINGS_MODEL_PATH:-/mnt/models/models--Qwen--Qwen3-Embedding-4B-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/Qwen3-Embedding-4B-Q8_0.gguf}"
      - "--port"
      - "${EMBEDDINGS_PORT:-8003}"
      - "--host"
      - "0.0.0.0"
      - "--jinja"
      - "--embeddings"
    ports:
      - "${EMBEDDINGS_PORT:-8003}:${EMBEDDINGS_PORT:-8003}"

  llm-instruct:
    <<: *llama-build
    volumes:
      - ${MODELS_DIR:-${HOME}/.cache/huggingface/hub}:/mnt/models
    environment:
      EXTRA_PARAMS: "${LLM_INSTRUCT_EXTRA_PARAMS-}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/app/llama-server-entrypoint.sh"]
    command:
      - "-m"
      - "${LLM_INSTRUCT_MODEL_PATH:-/mnt/models/models--ggml-org--gpt-oss-20b-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/gpt-oss-20b-mxfp4.gguf}"
      - "--port"
      - "${LLM_INSTRUCT_PORT:-8002}"
      - "--host"
      - "0.0.0.0"
      - "--ctx-size"
      - "${LLM_INSTRUCT_CTX_SIZE:-32768}"
      - "--n-predict"
      - "${LLM_INSTRUCT_N_PREDICT:-8192}"
      - "-b"
      - "${LLM_INSTRUCT_BATCH_SIZE:-512}"
      - "-ub"
      - "${LLM_INSTRUCT_UBATCH_SIZE:-512}"
      - "--n-gpu-layers"
      - "${LLM_INSTRUCT_N_GPU_LAYERS:-70}"
      - "--jinja"
    ports:
      - "${LLM_INSTRUCT_PORT:-8002}:${LLM_INSTRUCT_PORT:-8002}"

  llm-reasoning:
    <<: *llama-build
    volumes:
      - ${MODELS_DIR:-${HOME}/.cache/huggingface/hub}:/mnt/models
    environment:
      EXTRA_PARAMS: "${LLM_REASONING_EXTRA_PARAMS-}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/app/llama-server-entrypoint.sh"]
    command:
      - "-m"
      - "${LLM_REASONING_MODEL_PATH:-/mnt/models/models--mistralai--Ministral-3-14B-Reasoning-2512-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/Ministral-3-14B-Reasoning-2512-Q8_0.gguf}"
      - "--port"
      - "${LLM_REASONING_PORT:-8004}"
      - "--host"
      - "0.0.0.0"
      - "--ctx-size"
      - "${LLM_REASONING_CTX_SIZE:-32768}"
      - "--n-predict"
      - "${LLM_REASONING_N_PREDICT:-8192}"
      - "-b"
      - "${LLM_REASONING_BATCH_SIZE:-512}"
      - "-ub"
      - "${LLM_REASONING_UBATCH_SIZE:-512}"
      - "--n-gpu-layers"
      - "${LLM_REASONING_N_GPU_LAYERS:-999}"
      - "--jinja"
    ports:
      - "${LLM_REASONING_PORT:-8004}:${LLM_REASONING_PORT:-8004}"
