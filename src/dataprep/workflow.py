"""
Module de workflow pour le traitement des donnÃ©es avec les fonctions MCP DataPrep.
"""

import logging
from pathlib import Path

from ..config import get_config

# Import direct des fonctions MCP
from .mcp_functions import (
    download_and_store_url,
    get_knowledge_entries,
    upload_files_to_vectorstore,
)

logger = logging.getLogger(__name__)


def load_urls_from_file(config) -> list[str]:
    """Charge les URLs depuis le fichier configurÃ©."""
    urls_file_path = config.data.urls_file
    current_dir = Path(
        __file__
    ).parent.parent.parent  # src/dataprep -> src -> experiments/agentic-research
    urls_file = current_dir / urls_file_path

    if not urls_file.exists():
        raise FileNotFoundError(f"Fichier URLs non trouvÃ©: {urls_file}")

    urls = []
    with open(urls_file, encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            url = line.strip()
            if url and not url.startswith("#"):
                if url.startswith(("http://", "https://")):
                    urls.append(url)
                else:
                    logger.warning(f"URL invalide ignorÃ©e (ligne {line_num}): {url}")

    if not urls:
        raise ValueError(f"Aucune URL valide trouvÃ©e dans le fichier: {urls_file}")

    return urls


def analyze_knowledge_base(config):
    """Analyse l'Ã©tat actuel de la base de connaissances."""
    logger.info("=== ANALYSE DE LA BASE DE CONNAISSANCES ===")

    # Ã‰tat gÃ©nÃ©ral
    entries = get_knowledge_entries(config)

    logger.info(f"ðŸ“Š Total d'entrÃ©es: {len(entries)}")

    # Compter les fichiers indexÃ©s localement
    indexed_count = sum(1 for entry in entries if entry.get("vector_doc_id"))
    logger.info(f"ðŸ” Fichiers indexÃ©s localement: {indexed_count}")

    # VÃ©rifier les fichiers locaux
    local_dir = Path(config.data.local_storage_dir)
    local_files_count = 0
    if local_dir.exists():
        for entry in entries:
            local_file = local_dir / entry["filename"]
            if local_file.exists():
                local_files_count += 1

    logger.info(f"ðŸ“ Fichiers locaux disponibles: {local_files_count}")

    # DÃ©tails par entrÃ©e
    if entries:
        logger.info("\n=== DÃ‰TAILS DES ENTRÃ‰ES ===")
        for entry in entries:
            status_icons = []
            local_file = local_dir / entry["filename"] if local_dir.exists() else None
            if local_file and local_file.exists():
                status_icons.append("ðŸ“")
            if entry.get("vector_doc_id"):
                status_icons.append("ðŸ”")
            if not status_icons:
                status_icons.append("âŒ")

            status_str = " ".join(status_icons)
            title = entry.get("title", "Titre non disponible")
            logger.info(f"{status_str} {entry['filename']} - {title}")

            # Afficher le rÃ©sumÃ© s'il existe
            if entry.get("summary"):
                summary_preview = (
                    entry["summary"][:100] + "..."
                    if len(entry["summary"]) > 100
                    else entry["summary"]
                )
                logger.info(f"  ðŸ“ RÃ©sumÃ©: {summary_preview}")

    return entries


def run_workflow():
    """
    Fonction principale exÃ©cutant le workflow de traitement des donnÃ©es.
    """
    config = get_config()

    # Configuration du logging
    logging.basicConfig(level=getattr(logging, config.logging.level), format=config.logging.format)

    try:
        # 1. Analyser l'Ã©tat actuel de la base
        analyze_knowledge_base(config)

        # 2. Charger les URLs
        urls = load_urls_from_file(config)
        logger.info(f"\nDÃ©but du traitement de {len(urls)} URLs")

        # 3. TÃ©lÃ©charger et stocker chaque URL
        filenames = []
        for url in urls:
            try:
                filename = download_and_store_url(url, config)
                filenames.append(filename)
                logger.info(f"âœ… URL traitÃ©e: {url} -> {filename}")
            except Exception as e:
                logger.error(f"âŒ Erreur pour {url}: {e}")

        if not filenames:
            logger.error("Aucun fichier n'a pu Ãªtre traitÃ©")
            return

        # 4. Mode debug ou upload
        if config.debug.enabled:
            logger.info(f"\nMode debug activÃ© - {len(filenames)} fichiers stockÃ©s localement")

            # Afficher le contenu de la base de connaissances
            entries = get_knowledge_entries(config)

            logger.info("\n=== BASE DE CONNAISSANCES FINALE ===")
            for entry in entries:
                openai_status = "ðŸ” IndexÃ©" if entry.get("vector_doc_id") else "ðŸ“¥ Local"
                logger.info(f"ðŸ“„ {entry['filename']} ({openai_status})")
                logger.info(f"ðŸ”— Source: {entry['url']}")
                keywords = entry.get("keywords", [])
                if keywords:
                    logger.info(f"ðŸ·ï¸  Mots-clÃ©s LLM: {', '.join(keywords[:5])}")
                if entry.get("summary"):
                    logger.info(f"ðŸ“ RÃ©sumÃ©: {entry['summary'][:150]}...")
                if entry.get("vector_doc_id"):
                    logger.info(f"ðŸ†” Vector Doc ID: {entry['vector_doc_id']}")
                logger.info("---")

        else:
            # Mode normal: indexation locale avec optimisations
            logger.info("\nMode normal - indexation locale vers vector store")

            try:
                result = upload_files_to_vectorstore(
                    inputs=urls,  # Utiliser les URLs qui seront rÃ©solues
                    config=config,
                    vectorstore_name="agentic-research-vector-store",
                )

                logger.info("\n=== RAPPORT D'INDEXATION ===")
                logger.info(f"Vector Store ID: {result.vectorstore_id}")
                logger.info(f"Total de fichiers demandÃ©s: {result.total_files_requested}")
                logger.info(f"Nouvelles indexations: {result.upload_count}")
                logger.info(f"Fichiers rÃ©utilisÃ©s (dÃ©jÃ  indexÃ©s): {result.reuse_count}")

                logger.info("\n=== DÃ‰TAILS DES FICHIERS ===")
                logger.info("ðŸ” Indexations locales:")
                for file_info in result.files_uploaded:
                    status_icons = {"indexed": "ðŸ†•", "reused": "â™»ï¸", "failed": "âŒ"}
                    icon = status_icons.get(file_info["status"], "â“")
                    logger.info(
                        f"  {icon} {file_info['filename']} -> {file_info.get('doc_id', 'N/A')}"
                    )

            except Exception as e:
                logger.error(f"Erreur lors de l'upload: {e}")

    except Exception as e:
        logger.error(f"Erreur critique: {e}")
        raise


if __name__ == "__main__":
    run_workflow()
