"""
Module de workflow pour le traitement des donn√©es avec les fonctions MCP DataPrep.
"""

import logging
from pathlib import Path

from ..config import get_config

# Import direct des fonctions MCP
from .mcp_functions import (
    download_and_store_url,
    get_knowledge_entries,
    upload_files_to_vectorstore,
)

logger = logging.getLogger(__name__)


def load_urls_from_file(config) -> list[str]:
    """Charge les URLs depuis le fichier configur√©."""
    urls_file_path = config.data.urls_file
    current_dir = Path(
        __file__
    ).parent.parent.parent  # src/dataprep -> src -> experiments/agentic-research
    urls_file = current_dir / urls_file_path

    if not urls_file.exists():
        raise FileNotFoundError(f"Fichier URLs non trouv√©: {urls_file}")

    urls = []
    with open(urls_file, encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            url = line.strip()
            if url and not url.startswith("#"):
                if url.startswith(("http://", "https://")):
                    urls.append(url)
                else:
                    logger.warning(f"URL invalide ignor√©e (ligne {line_num}): {url}")

    if not urls:
        raise ValueError(f"Aucune URL valide trouv√©e dans le fichier: {urls_file}")

    return urls


def analyze_knowledge_base(config):
    """Analyse l'√©tat actuel de la base de connaissances."""
    logger.info("=== ANALYSE DE LA BASE DE CONNAISSANCES ===")

    # √âtat g√©n√©ral
    entries = get_knowledge_entries(config)

    logger.info(f"üìä Total d'entr√©es: {len(entries)}")

    # Compter les fichiers avec openai_file_id
    openai_files_count = sum(1 for entry in entries if entry.get("openai_file_id"))
    logger.info(f"‚òÅÔ∏è  Fichiers upload√©s sur OpenAI: {openai_files_count}")

    # V√©rifier les fichiers locaux
    local_dir = Path(config.data.local_storage_dir)
    local_files_count = 0
    if local_dir.exists():
        for entry in entries:
            local_file = local_dir / entry["filename"]
            if local_file.exists():
                local_files_count += 1

    logger.info(f"üìÅ Fichiers locaux disponibles: {local_files_count}")

    # D√©tails par entr√©e
    if entries:
        logger.info("\n=== D√âTAILS DES ENTR√âES ===")
        for entry in entries:
            status_icons = []
            local_file = local_dir / entry["filename"] if local_dir.exists() else None
            if local_file and local_file.exists():
                status_icons.append("üìÅ")
            if entry.get("openai_file_id"):
                status_icons.append("‚òÅÔ∏è")
            if not status_icons:
                status_icons.append("‚ùå")

            status_str = " ".join(status_icons)
            title = entry.get("title", "Titre non disponible")
            logger.info(f"{status_str} {entry['filename']} - {title}")

            # Afficher le r√©sum√© s'il existe
            if entry.get("summary"):
                summary_preview = (
                    entry["summary"][:100] + "..."
                    if len(entry["summary"]) > 100
                    else entry["summary"]
                )
                logger.info(f"  üìù R√©sum√©: {summary_preview}")

    return entries


def run_workflow():
    """
    Fonction principale ex√©cutant le workflow de traitement des donn√©es.
    """
    config = get_config()

    # Configuration du logging
    logging.basicConfig(level=getattr(logging, config.logging.level), format=config.logging.format)

    try:
        # 1. Analyser l'√©tat actuel de la base
        analyze_knowledge_base(config)

        # 2. Charger les URLs
        urls = load_urls_from_file(config)
        logger.info(f"\nD√©but du traitement de {len(urls)} URLs")

        # 3. T√©l√©charger et stocker chaque URL
        filenames = []
        for url in urls:
            try:
                filename = download_and_store_url(url, config)
                filenames.append(filename)
                logger.info(f"‚úÖ URL trait√©e: {url} -> {filename}")
            except Exception as e:
                logger.error(f"‚ùå Erreur pour {url}: {e}")

        if not filenames:
            logger.error("Aucun fichier n'a pu √™tre trait√©")
            return

        # 4. Mode debug ou upload
        if config.debug.enabled:
            logger.info(f"\nMode debug activ√© - {len(filenames)} fichiers stock√©s localement")

            # Afficher le contenu de la base de connaissances
            entries = get_knowledge_entries(config)

            logger.info("\n=== BASE DE CONNAISSANCES FINALE ===")
            for entry in entries:
                openai_status = "üì§ Upload√©" if entry.get("openai_file_id") else "üì• Local"
                logger.info(f"üìÑ {entry['filename']} ({openai_status})")
                logger.info(f"üîó Source: {entry['url']}")
                keywords = entry.get("keywords", [])
                if keywords:
                    logger.info(f"üè∑Ô∏è  Mots-cl√©s LLM: {', '.join(keywords[:5])}")
                if entry.get("summary"):
                    logger.info(f"üìù R√©sum√©: {entry['summary'][:150]}...")
                if entry.get("openai_file_id"):
                    logger.info(f"üÜî OpenAI File ID: {entry['openai_file_id']}")
                logger.info("---")

        else:
            # Mode normal: upload vers vector store avec optimisations
            logger.info("\nMode normal - upload optimis√© vers vector store")

            try:
                result = upload_files_to_vectorstore(
                    inputs=urls,  # Utiliser les URLs qui seront r√©solues
                    config=config,
                    vectorstore_name="agentic-research-vector-store",
                )

                logger.info("\n=== RAPPORT D'UPLOAD OPTIMIS√â ===")
                logger.info(f"Vector Store ID: {result.vectorstore_id}")
                logger.info(f"Total de fichiers demand√©s: {result.total_files_requested}")
                logger.info(f"Nouveaux uploads vers OpenAI: {result.upload_count}")
                logger.info(f"Fichiers r√©utilis√©s (d√©j√† sur OpenAI): {result.reuse_count}")
                logger.info(f"Attachements r√©ussis au vector store: {result.attach_success_count}")
                logger.info(f"√âchecs d'attachement: {result.attach_failure_count}")

                logger.info("\n=== D√âTAILS DES FICHIERS ===")
                logger.info("üì§ Uploads vers OpenAI Files API:")
                for file_info in result.files_uploaded:
                    status_icons = {"uploaded": "üÜï", "reused": "‚ôªÔ∏è", "failed": "‚ùå"}
                    icon = status_icons.get(file_info["status"], "‚ùì")
                    logger.info(
                        f"  {icon} {file_info['filename']} -> {file_info.get('file_id', 'N/A')}"
                    )

                logger.info("\nüìé Attachements au Vector Store:")
                for file_info in result.files_attached:
                    status_icon = "‚úÖ" if file_info["status"] == "attached" else "‚ùå"
                    logger.info(f"  {status_icon} {file_info['filename']}")

            except Exception as e:
                logger.error(f"Erreur lors de l'upload: {e}")

    except Exception as e:
        logger.error(f"Erreur critique: {e}")
        raise


if __name__ == "__main__":
    run_workflow()
