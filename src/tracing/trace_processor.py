import logging
import logging.handlers
import queue
from datetime import datetime
from pathlib import Path
from typing import Any

from agents import Span, Trace, TracingProcessor


class FileTraceProcessor(TracingProcessor):
    """
    Processeur de tracing am√©lior√© qui log tous les √©v√©nements dans un fichier avec rotation.

    Am√©liorations :
    - Ne pollue pas la console (log uniquement dans le fichier)
    - Rotation automatique des fichiers (max 10 fichiers de 10MB chacun)
    - Limite le nombre de logs par fichier
    - Nettoyage automatique des anciens fichiers
    """

    def __init__(
        self,
        log_dir: str = "traces",
        log_file: str = None,
        max_files: int = 10,
        max_bytes: int = 10 * 1024 * 1024,  # 10MB
        backup_count: int = 5,
    ):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        if log_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = f"trace_log_{timestamp}.log"

        self.log_file_path = self.log_dir / log_file
        self.max_files = max_files
        self.max_bytes = max_bytes
        self.backup_count = backup_count

        # Logger asynchrone pour performance - SANS console output
        self.logger = logging.getLogger("FileTraceProcessor")

        # IMPORTANT: D√©sactiver la propagation vers la console
        self.logger.propagate = False

        # Queue pour logging asynchrone
        self.log_queue = queue.Queue()

        # Handler avec rotation automatique (comme log4j)
        file_handler = logging.handlers.RotatingFileHandler(
            self.log_file_path,
            maxBytes=self.max_bytes,
            backupCount=self.backup_count,
            encoding="utf-8",
        )
        self._file_handler = file_handler

        # Formatter plus compact
        formatter = logging.Formatter(
            "%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
        )
        file_handler.setFormatter(formatter)

        # Queue handler pour performance (non-bloquant)
        queue_handler = logging.handlers.QueueHandler(self.log_queue)

        # Listener qui g√®re l'√©criture en arri√®re-plan
        self.queue_listener = logging.handlers.QueueListener(
            self.log_queue, file_handler, respect_handler_level=True
        )
        self.queue_listener.start()

        # Nettoyer les anciens fichiers de trace
        self._cleanup_old_trace_files()

        if not self.logger.handlers:
            self.logger.addHandler(queue_handler)
            self.logger.setLevel(logging.INFO)

    def _cleanup_old_trace_files(self) -> None:
        """
        Nettoie les anciens fichiers de trace pour √©viter l'accumulation.
        Garde seulement les max_files plus r√©cents.
        """
        try:
            # Trouver tous les fichiers de trace
            trace_files = list(self.log_dir.glob("trace_log_*.log*"))

            if len(trace_files) > self.max_files:
                # Trier par date de modification (plus r√©cent en premier)
                trace_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

                # Supprimer les fichiers en trop
                files_to_delete = trace_files[self.max_files :]
                for file_path in files_to_delete:
                    try:
                        file_path.unlink()
                        # Ne pas polluer la console: pas de print ici
                    except Exception as e:
                        # Ne pas polluer la console: ignorer silencieusement
                        _ = e
        except Exception as e:
            # Ne pas polluer la console: ignorer silencieusement
            _ = e

    def _safe_export_data(self, export_func) -> str:
        """
        Exporte les donn√©es de mani√®re s√©curis√©e en cha√Æne de caract√®res.
        """
        try:
            exported_data = export_func()
            return str(exported_data)
        except Exception as e:
            return f"<Export failed: {type(e).__name__}: {e}>"

    def _log_event(self, data: dict) -> None:
        """
        Log un √©v√©nement en format simple et lisible.
        """
        try:
            # Format plus compact et lisible
            event_str = f"EVENT: {data['event']}"
            if "trace_id" in data:
                event_str += f" | TRACE: {data['trace_id']}"
            if "span_id" in data:
                event_str += f" | SPAN: {data['span_id']}"
            if "name" in data:
                event_str += f" | NAME: {data['name']}"
            if "started_at" in data:
                event_str += f" | STARTED: {data['started_at']}"
            if "ended_at" in data:
                event_str += f" | ENDED: {data['ended_at']}"
            if "exported_data" in data:
                # Limiter la taille des donn√©es export√©es pour √©viter les logs trop longs
                exported_str = str(data["exported_data"])
                if len(exported_str) > 500:
                    exported_str = exported_str[:500] + "..."
                event_str += f" | DATA: {exported_str}"

            self.logger.info(event_str)
        except Exception as e:
            # En cas d'√©chec, log brut
            self.logger.error(f"CRITICAL: Failed to log event. Error: {e}. Raw data: {data}")

    def on_trace_start(self, trace: Trace) -> None:
        data = {
            "event": "trace_start",
            "trace_id": trace.trace_id,
            "name": trace.name,
            "exported_data": self._safe_export_data(trace.export),
        }
        self._log_event(data)

    def on_trace_end(self, trace: Trace) -> None:
        data = {
            "event": "trace_end",
            "trace_id": trace.trace_id,
            "name": trace.name,
            "exported_data": self._safe_export_data(trace.export),
        }
        self._log_event(data)

    def on_span_start(self, span: Span[Any]) -> None:
        data = {
            "event": "span_start",
            "span_id": span.span_id,
            "trace_id": span.trace_id,
            "parent_id": span.parent_id,
            "started_at": str(span.started_at) if span.started_at else None,
            "exported_data": self._safe_export_data(span.export),
        }
        self._log_event(data)

    def on_span_end(self, span: Span[Any]) -> None:
        data = {
            "event": "span_end",
            "span_id": span.span_id,
            "trace_id": span.trace_id,
            "parent_id": span.parent_id,
            "started_at": str(span.started_at) if span.started_at else None,
            "ended_at": str(span.ended_at) if span.ended_at else None,
            "exported_data": self._safe_export_data(span.export),
        }
        self._log_event(data)

    def get_log_file_path(self) -> Path:
        """
        Retourne le chemin du fichier de log actuel.
        Utile pour consulter les logs quand n√©cessaire.
        """
        return self.log_file_path

    def show_recent_logs(self, lines: int = 20) -> None:
        """
        Affiche les logs r√©cents dans la console pour debug.
        """
        try:
            if self.log_file_path.exists():
                with open(self.log_file_path, encoding="utf-8") as f:
                    all_lines = f.readlines()
                    recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
                    print(f"\nüìã {lines} derniers logs de {self.log_file_path.name}:")
                    print("=" * 80)
                    for line in recent_lines:
                        print(line.rstrip())
                    print("=" * 80)
            else:
                print(f"üìã Aucun fichier de log trouv√©: {self.log_file_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la lecture des logs: {e}")

    def shutdown(self, timeout: float | None = None) -> None:
        # Arr√™ter le listener asynchrone proprement
        if hasattr(self, "queue_listener"):
            self.queue_listener.stop()

        for handler in self.logger.handlers:
            handler.close()
            self.logger.removeHandler(handler)

    def force_flush(self) -> None:
        # Force un flush best-effort vers le disque sans risquer de bloquer sur Queue.join()
        if hasattr(self, "_file_handler") and hasattr(self._file_handler, "flush"):
            self._file_handler.flush()
