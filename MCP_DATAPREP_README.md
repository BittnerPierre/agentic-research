# Module MCP DataPrep

SystÃ¨me de gestion de base de connaissances locale avec upload optimisÃ© vers OpenAI Vector Store.

## ğŸ¯ FonctionnalitÃ©s

### 1. Gestion de Base de Connaissances Locale

- **Base JSON thread-safe** avec verrous de fichiers (portalocker)
- **Stockage local** des fichiers `.md` avec mÃ©tadonnÃ©es
- **Extraction de mots-clÃ©s** intelligente via LLM
- **Lookup rapide** par URL ou nom de fichier

### 2. Upload OptimisÃ© vers OpenAI

- **RÃ©utilisation** des fichiers dÃ©jÃ  uploadÃ©s (OpenAI File ID)
- **Upload en parallÃ¨le** avec gestion d'erreurs
- **Vector Store** temporaire (expiration 1 jour)
- **MÃ©triques dÃ©taillÃ©es** des opÃ©rations

### 3. Interface MCP

- **3 outils MCP** pour les agents
- **Serveur FastMCP** intÃ©grÃ©
- **Configuration YAML** centralisÃ©e

## ğŸ“¦ Installation

```bash
cd experiments/agentic-research
poetry install
```

### DÃ©pendances ajoutÃ©es

- `portalocker` : Verrous de fichiers thread-safe

## âš™ï¸ Configuration

### `configs/config-default.yaml`

```yaml
data:
  urls_file: "urls.txt" # URLs existantes (lecture seule)
  knowledge_db_path: "data/knowledge_db.json" # Base de connaissances
  local_storage_dir: "data/" # Stockage fichiers .md
```

## ğŸš€ Utilisation

### 1. Script de Workflow (Compatible avec l'existant)

```bash
# Reproduit dataprep.core:main avec les nouvelles fonctionnalitÃ©s
poetry run mcp-dataprep-workflow
```

### 2. Serveur MCP

```bash
# DÃ©marrer le serveur MCP DataPrep
poetry run dataprep_server
```

Le serveur sera accessible sur `http://localhost:8001` avec transport SSE.

### 3. Utilisation Directe des Fonctions

```python
from src.dataprep.mcp_functions import download_and_store_url, upload_files_to_vectorstore, get_knowledge_entries
from src.config import get_config

config = get_config()

# 1. TÃ©lÃ©charger une URL
filename = download_and_store_url("https://example.com/article", config)

# 2. Consulter la base de connaissances
entries = get_knowledge_entries(config)
print(f"EntrÃ©es disponibles: {len(entries)}")

# 3. Upload vers vector store
result = upload_files_to_vectorstore(
    inputs=["https://example.com/article"],  # URLs ou noms de fichiers
    config=config,
    vectorstore_name="my-research"
)
print(f"Vector Store ID: {result.vectorstore_id}")
print(f"Fichiers rÃ©utilisÃ©s: {result.reuse_count}")
```

## ğŸ”§ Outils MCP Disponibles

### `download_and_store_url_tool(url: str) -> str`

TÃ©lÃ©charge et stocke une URL dans la base de connaissances locale.

**ParamÃ¨tres:**

- `url`: URL Ã  tÃ©lÃ©charger

**Retour:** Nom du fichier local crÃ©Ã©

**Comportement:**

- Lookup dans la base existante
- Si trouvÃ© â†’ retourne le nom de fichier
- Sinon â†’ tÃ©lÃ©charge, convertit en Markdown, extrait mots-clÃ©s LLM, stocke

### `upload_files_to_vectorstore_tool(inputs: List[str], vectorstore_name: str) -> Dict`

Upload optimisÃ© vers OpenAI Vector Store.

**ParamÃ¨tres:**

- `inputs`: Liste d'URLs ou noms de fichiers
- `vectorstore_name`: Nom du vector store Ã  crÃ©er

**Retour:** RÃ©sultat dÃ©taillÃ© avec mÃ©triques

**Optimisations:**

- RÃ©utilise les `openai_file_id` existants
- Upload uniquement les nouveaux fichiers
- Vector store avec expiration 1 jour

### `get_knowledge_entries_tool() -> List[Dict]`

Consulte l'index de la base de connaissances.

**Retour:** Liste des entrÃ©es avec URL, nom de fichier, titre, mots-clÃ©s, ID OpenAI

## ğŸ“Š Structure des DonnÃ©es

### KnowledgeEntry

```python
{
    "url": "https://example.com/article",
    "filename": "article.md",
    "title": "Titre du document",
    "keywords": ["AI", "Machine Learning"],  # Extraits par LLM
    "openai_file_id": "file_123abc",         # Optimisation uploads
    "created_at": "2025-01-07T10:30:00",
    "last_uploaded_at": "2025-01-07T11:00:00"
}
```

### UploadResult

```python
{
    "vectorstore_id": "vs_abc123",
    "total_files_requested": 5,
    "upload_count": 2,           # Nouveaux uploads
    "reuse_count": 3,            # Fichiers rÃ©utilisÃ©s
    "attach_success_count": 5,
    "attach_failure_count": 0,
    "files_uploaded": [...],
    "files_attached": [...]
}
```

## ğŸ”„ Workflow Agentique RecommandÃ©

### 1. Planner Agent

```python
# Consulter la base de connaissances
entries = mcp_dataprep.get_knowledge_entries_tool()

# Identifier les URLs manquantes
available_urls = {entry['url'] for entry in entries}
syllabus_urls = ["https://...", "https://..."]
urls_to_download = [url for url in syllabus_urls if url not in available_urls]

if urls_to_download:
    print(f"URLs Ã  tÃ©lÃ©charger: {urls_to_download}")
```

### 2. Documentalist Agent

```python
# TÃ©lÃ©charger les URLs manquantes
for url in urls_to_download:
    filename = mcp_dataprep.download_and_store_url_tool(url)
    print(f"TÃ©lÃ©chargÃ©: {url} -> {filename}")
```

### 3. Upload vers Vector Store

```python
# Upload optimisÃ© (rÃ©utilise les fichiers existants)
result = mcp_dataprep.upload_files_to_vectorstore_tool(
    inputs=syllabus_urls,  # Toutes les URLs (optimisation automatique)
    vectorstore_name="research-session-123"
)

print(f"Vector Store: {result['vectorstore_id']}")
print(f"Optimisation: {result['reuse_count']}/{result['total_files_requested']} fichiers rÃ©utilisÃ©s")
```

## ğŸ§ª Tests

```bash
# Tests d'intÃ©gration
cd experiments/agentic-research
python -m pytest integration_tests/test_mcp_dataprep.py -v

# Test avec mocks
python integration_tests/test_mcp_dataprep.py
```

## ğŸ“ Architecture des Fichiers

```
src/
â”œâ”€â”€ dataprep/
â”‚   â”œâ”€â”€ models.py              # SchÃ©mas Pydantic
â”‚   â”œâ”€â”€ knowledge_db.py        # Gestionnaire thread-safe
â”‚   â”œâ”€â”€ mcp_functions.py       # 3 fonctions MCP principales
â”‚   â””â”€â”€ core.py               # Fonctions legacy (intactes)
â”œâ”€â”€ mcp/
â”‚   â””â”€â”€ dataprep_server.py    # Serveur MCP FastMCP
â””â”€â”€ config.py                 # Configuration Ã©tendue

scripts/
â””â”€â”€ mcp_dataprep_workflow.py  # Script compatible existant

integration_tests/
â””â”€â”€ test_mcp_dataprep.py      # Tests d'intÃ©gration

data/                          # Stockage local
â”œâ”€â”€ knowledge_db.json          # Base de connaissances
â””â”€â”€ *.md                      # Fichiers markdown
```

## ğŸ”’ SÃ©curitÃ© et Performance

### Thread Safety

- **Verrous de fichiers** avec `portalocker`
- **Pattern read-merge-write** atomique
- **Gestion des erreurs** de concurrence

### Optimisation Memory/CPU

- **Streaming upload** pour gros fichiers
- **RÃ©utilisation** des uploads OpenAI
- **Extraction LLM** avec fallback

### Logging StructurÃ©

```python
logger.info("Document tÃ©lÃ©chargÃ©", extra={
    "url": url,
    "filename": filename,
    "content_length": len(content),
    "keywords_count": len(keywords)
})
```

## ğŸš€ Migration depuis l'Existant

Le module est **100% compatible** avec l'existant :

- `src.dataprep.core` : **InchangÃ©**
- `poetry run dataprep` : **Fonctionne toujours**
- Nouvelles fonctionnalitÃ©s : **Additives uniquement**

### Script de Migration

```bash
# Ancien workflow
poetry run dataprep

# Nouveau workflow avec optimisations
poetry run mcp-dataprep-workflow
```

## ğŸ“ˆ MÃ©triques et Monitoring

Le systÃ¨me fournit des mÃ©triques dÃ©taillÃ©es :

- **Optimisation ratio** : `reuse_count / total_files`
- **Performance upload** : Temps par fichier
- **Base de connaissances** : Croissance, mots-clÃ©s frÃ©quents
- **Vector store** : Taux de succÃ¨s/Ã©chec

## ğŸ”„ Roadmap

### Phase Actuelle âœ…

- [x] Base de connaissances thread-safe
- [x] Upload optimisÃ© OpenAI
- [x] Interface MCP 3 outils
- [x] Scripts de workflow
- [x] Tests d'intÃ©gration

### AmÃ©liorations Futures

- [ ] Interface web pour visualiser la base
- [ ] Synchronisation multi-machines
- [ ] Analytics avancÃ©es des mots-clÃ©s
- [ ] Support d'autres sources (PDF, etc.)

---

**ğŸ† Le module MCP DataPrep optimise significativement les workflows de recherche agentique en Ã©liminant les tÃ©lÃ©chargements et uploads redondants !**
