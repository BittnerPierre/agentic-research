# Configuration pour agentic-research (Docker DGX)
config_name: "docker-dgx"

vector_store:
  name: "agentic-research-dgx"
  description: "Vector store for DGX Docker stack"
  expires_after_days: 30

vector_search:
  provider: "chroma"
  index_name: "agentic-research-dgx"
  chunk_size: 800
  chunk_overlap: 120
  chroma_host: "chromadb"
  chroma_port: 8000
  chroma_ssl: false
  # Uses Chroma OpenAIEmbeddingFunction, but the api_base points to llama.cpp embeddings (not OpenAI).
  chroma_embedding_provider: "openai"
  chroma_embedding_api_base: "http://embeddings-gpu:8003/v1"
  chroma_embedding_model: "Qwen3-Embedding-4B-Q8_0.gguf"
  chroma_embedding_api_key_env: "CHROMA_OPENAI_API_KEY"
  top_k: 5
  score_threshold: null

# Configuration des données d'entrée
data:
  urls_file: "urls.txt"
  knowledge_db_path: "data/knowledge_db.json"
  local_storage_dir: "data/"

dataprep:
  llm:
    enabled: true
    model:
      name: "openai/sm"
      base_url: "http://llm-instruct:8002/v1"
      api_key: "dummy"
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.1
    max_tokens: 256
    timeout_seconds: 30.0

# Configuration de debug
debug:
  enabled: false
  output_dir: "debug_output"
  save_reports: true
  show_content_preview: true

# Configuration du logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Configuration MCP (Model Context Protocol)
mcp:
  server_host: "0.0.0.0"
  server_port: 8001
  client_timeout_seconds: 60.0
  http_timeout_seconds: 5.0

# Configuration Models
models:
  research_model:
    name: "openai/rm"
    base_url: "http://llm-instruct:8002"
    api_key: "dummy"
  planning_model:
    name: "openai/pm"
    base_url: "http://llm-reasoning:8004"
    api_key: "dummy"
  search_model:
    name: "openai/sm"
    base_url: "http://llm-instruct:8002"
    api_key: "dummy"
  writer_model:
    name: "openai/wm"
    base_url: "http://llm-instruct:8002"
    api_key: "dummy"
  knowledge_preparation_model:
    name: "openai/kpm"
    base_url: "http://llm-instruct:8002"
    api_key: "dummy"

# Configuration du manager
manager:
  default_manager: "deep_manager"

agents:
  max_search_plan: "3-7"
  output_dir: "output/"
  writer_output_format: "markdown"
  file_search_rewrite_mode: "none" # none | paraphrase_lite | hyde_lite (experimental, can add latency)
  file_search_rewrite_max_variants: 2
  file_search_top_k: null
  file_search_score_threshold: null
