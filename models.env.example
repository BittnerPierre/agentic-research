# Paths are mounted into the DGX containers. Replace REPLACE_WITH_SNAPSHOT accordingly.
# MODELS_DIR defaults to the HuggingFace cache, but can be overridden for DGX mounts.
MODELS_DIR=${HOME}/Library/Caches/llama.cpp/

# Local CPU stack
LOCAL_LLM_MODEL_PATH=unsloth_Qwen3-4B-Instruct-2507-GGUF_Qwen3-4B-Instruct-2507-Q4_K_M.gguf

# Embeddings (service: embeddings-gpu)
EMBEDDINGS_MODEL_PATH=/mnt/models/models--Qwen--Qwen3-Embedding-4B-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/Qwen3-Embedding-4B-Q8_0.gguf

# LLM Instruct (service: llm-instruct) - used for research/planning/search/knowledge_preparation
LLM_INSTRUCT_MODEL_PATH=/mnt/models/models--ggml-org--gpt-oss-20b-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/gpt-oss-20b-mxfp4.gguf
LLM_INSTRUCT_CTX_SIZE=32768
LLM_INSTRUCT_N_PREDICT=8192
LLM_INSTRUCT_BATCH_SIZE=512
LLM_INSTRUCT_UBATCH_SIZE=512

# LLM Reasoning (service: llm-reasoning) - used for writer
LLM_REASONING_MODEL_PATH=/mnt/models/models--mistralai--Ministral-3-14B-Reasoning-2512-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/Ministral-3-14B-Reasoning-2512-Q8_0.gguf
LLM_REASONING_CTX_SIZE=32768
LLM_REASONING_N_PREDICT=8192
LLM_REASONING_BATCH_SIZE=512
LLM_REASONING_UBATCH_SIZE=512
