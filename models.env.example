# Paths are mounted into the DGX containers. Replace REPLACE_WITH_SNAPSHOT accordingly.
# MODELS_DIR defaults to the HuggingFace cache, but can be overridden for DGX mounts.
# MODELS_DIR must point to the hub/ directory (mounted as /mnt/models in Docker)
MODELS_DIR=${HOME}/.cache/huggingface/hub

# Optional Extra Parameters
# ========================
# For models requiring additional llama.cpp parameters (e.g., Mistral-Small-3.2-24B-Instruct),
# set the appropriate *_EXTRA_PARAMS variables below. These are appended to the llama.cpp
# command line when the service starts.
#
# Example for Mistral-Small-3.2-24B-Instruct:
# LLM_INSTRUCT_EXTRA_PARAMS="--temp 0.15 --top-k -1 --top-p 1.00 -ngl 99"
# LLM_REASONING_EXTRA_PARAMS="--temp 0.2 --top-p 0.95"
#
# For models like GPT-OSS that don't need extra parameters, leave these empty:

# Local CPU stack
LOCAL_LLM_MODEL_PATH=unsloth_Qwen3-4B-Instruct-2507-GGUF_Qwen3-4B-Instruct-2507-Q4_K_M.gguf

# Embeddings (service: embeddings-gpu)
EMBEDDINGS_MODEL_PATH=/mnt/models/models--Qwen--Qwen3-Embedding-4B-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/Qwen3-Embedding-4B-Q8_0.gguf
# Optional extra parameters for llama.cpp embeddings service
# Leave empty if not needed
EMBEDDINGS_EXTRA_PARAMS=""

# LLM Instruct (service: llm-instruct) - used for research/planning/search/knowledge_preparation
LLM_INSTRUCT_MODEL_PATH=/mnt/models/models--ggml-org--gpt-oss-20b-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/gpt-oss-20b-mxfp4.gguf
LLM_INSTRUCT_CTX_SIZE=32768
LLM_INSTRUCT_N_PREDICT=8192
LLM_INSTRUCT_BATCH_SIZE=512
LLM_INSTRUCT_UBATCH_SIZE=512
# Optional extra parameters for llama.cpp (e.g., for Mistral-Small-3.2-24B-Instruct)
# Leave empty if not needed
LLM_INSTRUCT_EXTRA_PARAMS=""

# LLM Reasoning (service: llm-reasoning) - used for writer
LLM_REASONING_MODEL_PATH=/mnt/models/models--mistralai--Ministral-3-14B-Reasoning-2512-GGUF/snapshots/REPLACE_WITH_SNAPSHOT/Ministral-3-14B-Reasoning-2512-Q8_0.gguf
LLM_REASONING_CTX_SIZE=32768
LLM_REASONING_N_PREDICT=8192
LLM_REASONING_BATCH_SIZE=512
LLM_REASONING_UBATCH_SIZE=512
# Optional extra parameters for llama.cpp (e.g., for Mistral-Small-3.2-24B-Instruct)
# Leave empty if not needed
LLM_REASONING_EXTRA_PARAMS=""
