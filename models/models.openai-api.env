##
## OPENAI API (benchmark metadata + local services parity with OPENAI)
##

MODELS_DIR=${HOME}/.cache/huggingface/hub

LLM_INSTRUCT_MODEL_PATH=/mnt/models/models--ggml-org--gpt-oss-20b-GGUF/snapshots/e1dc459feff949ff451ce107337a2026daa80df8/gpt-oss-20b-mxfp4.gguf
#LLM_INSTRUCT_MODEL_PATH=/mnt/models/models--ggml-org--gpt-oss-120b-GGUF/snapshots/d932fcea62f83e088d8f076a2cd2d7eb02dfa682/gpt-oss-120b-mxfp4-00001-of-00003.gguf
LLM_INSTRUCT_EXTRA_PARAMS="--parallel 2 --flash-attn on"
# LLM_INSTRUCT_CTX_SIZE=65536
LLM_INSTRUCT_CTX_SIZE=32768
# LLM_INSTRUCT_CTX_SIZE=0
# LLM_INSTRUCT_N_PREDICT=8192
LLM_INSTRUCT_BATCH_SIZE=256 # 512
LLM_INSTRUCT_UBATCH_SIZE=256 # 512
# LLM_INSTRUCT_N_GPU_LAYERS=70

LLM_REASONING_MODEL_PATH=/mnt/models/models--ggml-org--gpt-oss-20b-GGUF/snapshots/e1dc459feff949ff451ce107337a2026daa80df8/gpt-oss-20b-mxfp4.gguf

LLM_REASONING_EXTRA_PARAMS="--parallel 1 --flash-attn on --chat-template-kwargs {\"reasoning_effort\":\"medium\"}"
LLM_REASONING_CTX_SIZE=12288 #32768
# LLM_REASONING_CTX_SIZE=0
LLM_REASONING_N_PREDICT=2048 # 8192
LLM_REASONING_BATCH_SIZE=128 #512
LLM_REASONING_UBATCH_SIZE=128 #512
# LLM_REASONING_N_GPU_LAYERS=999


EMBEDDINGS_MODEL_PATH=/mnt/models/models--Qwen--Qwen3-Embedding-4B-GGUF/snapshots/f4602530db1d980e16da9d7d3a70294cf5c190be/Qwen3-Embedding-4B-Q8_0.gguf

# FOR DOWNLOAD

LLM_INSTRUCT_MODEL_REPO=ggml-org/gpt-oss-20b-GGUF
INSTRUCT_GGUF_PATTERN=gpt-oss-20b-mxfp4.gguf

#LLM_INSTRUCT_MODEL_REPO=ggml-org/gpt-oss-120b-GGUF
#INSTRUCT_GGUF_PATTERN=gpt-oss-120b-mxfp4-*.gguf

LLM_REASONING_MODEL_REPO=ggml-org/gpt-oss-20b-GGUF
REASONING_GGUF_PATTERN=gpt-oss-20b-mxfp4.gguf

EMBEDDINGS_MODEL_REPO=Qwen/Qwen3-Embedding-4B-GGUF
EMBEDDINGS_GGUF_PATTERN=Qwen3-Embedding-4B-Q8_0.gguf
