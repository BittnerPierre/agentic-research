services:
  dataprep:
    build:
      context: .
      dockerfile: docker/Dockerfile.dataprep
    ports:
      - "8001:8001"
    env_file:
      - .env
    volumes:
      - ./config.yaml:/app/config.yaml
      - ./urls.txt:/app/urls.txt
      - ./data:/app/data
      - ./output:/app/output
      - ./logs:/app/logs
    restart: unless-stopped

  agentic-research:
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
    env_file:
      - .env
    environment:
      MCP_DATAPREP_URL: http://dataprep:8001/sse
      MCP_FS_COMMAND: node
      MCP_FS_ARGS: /usr/local/lib/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js
    volumes:
      - ./config.yaml:/app/config.yaml
      - ./urls.txt:/app/urls.txt
      - ./data:/app/data
      - ./output:/app/output
      - ./logs:/app/logs
    stdin_open: true
    tty: true
    profiles:
      - cli

  evaluations:
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
    env_file:
      - .env
    environment:
      MCP_DATAPREP_URL: http://dataprep:8001/sse
      MCP_FS_COMMAND: node
      MCP_FS_ARGS: /usr/local/lib/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js
    volumes:
      - ./config.yaml:/app/config.yaml
      - ./urls.txt:/app/urls.txt
      - ./data:/app/data
      - ./output:/app/output
      - ./logs:/app/logs
      - ./evaluations:/app/evaluations
      - ./test_files:/app/test_files
    profiles:
      - eval

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - ./data/chromadb:/chroma/chroma
    environment:
      ANONYMIZED_TELEMETRY: "False"
    profiles:
      - v1-local
      - v1-dgx
      - v1-dgx-backend

  embeddings-cpu:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-latest
    platform: linux/amd64
    ports:
      - "8003:8003"
    volumes:
      - ./data/embeddings:/data
    profiles:
      - v1-local

  embeddings-gpu:
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    ports:
      - "8003:8003"
    volumes:
      - ./data/embeddings:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - v1-dgx
      - v1-dgx-backend

  llama-cpp-cpu:
    image: ghcr.io/ggml-org/llama.cpp:full
    platform: linux/amd64
    ports:
      - "8002:8002"
    volumes:
      - ./models:/models
    profiles:
      - v1-local

  llama-cpp-gpu:
    image: ghcr.io/ggml-org/llama.cpp:full
    ports:
      - "8002:8002"
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - v1-dgx
